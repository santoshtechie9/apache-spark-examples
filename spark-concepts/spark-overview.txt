
Apache Spark:
============

What Is Apache Spark?
Apache Spark is a cluster computing platform designed to be fast and general-purpose.
Spark extends the popular MapReduce model. It can run computations in memory and disk.
It called general purpose as it supprots batch applications, iterative algorithms, interactive queries, and streaming. These different type of applications require various distributed systems earlier.

Spark is designed to be highly accessible, you can develop spark jobs in Python, Java, Scala,and SQL.
Spark integrates well with other big data tool. Spark can run on Hadoop and can access any Hadoop data source.

At its core, Spark is a “computational engine” that is responsible for scheduling, distributing, and monitoring applications consisting of many computational tasks across many worker
machines, or a computing cluster.

Why spark ?
- fast and general engine for large scale processing
- open source cluster computing framework
- end to end analytics platform 
- overcomes limitation of hadoop/mapreduce
- runs on cluster
- iterative and intecatvie and stream processing 
- supportws multiple languages scala, java, python and R
- major companies like Google, Amazon and other use spark

When to use spark?
- Spark is centered around data processing
- Data integration and ETL
- Interactive analytics
- Batch computing
- stream processing

Spark workflow:
1.Load data from source
2.transform data
3.store processed data
4.interactive analytics
5.machine learning 
6.data driven decision making
 

Spark Architecture:
==================

Spark core: 
It contains the basic functionality of spark.
Basic functionalities of spark code include task scheduling, memory management, fault recovery and interacting with storage systems and more.
Spark core is also the home to the API that define RDDs.

RDD:
RDD stands for resilient distributed datasets.
RDDs are Spark’s fundamental abstraction for distributed data and computation.
RDDs represent a collection of items distributed across a cluster of computers that can be manipulated parallely.
RDDs allow us to programatically manipulate the data in languages like scala, java and python.

Spark SQL:
Spark SQL allows queryig data sets via SQL. Spark SQL is the spark's package for working with structured data.

Spark Streaming:
Spark streaming is spark's module for working with steam of data.
It provids APIs for manipulating data stream.
These APIs are very much similar to spark core RDD APIs.

Spark MLIB:

Spark GraphX:
Sparks graphx module provides APIs for processing graphs.

Cluster Managers:
Spark integrates well with other big data tools like Hadoop. It can run on existing Hadoop cluster.
Spark comes with an in-built scheduler called "Spark scheduler". 
It can also run on other cluster managers like Hadoop YARN and Apache Mesos.

